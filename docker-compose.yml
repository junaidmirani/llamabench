version: '3.8'

services:
  # llama.cpp server
  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: llamabench-llamacpp
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models
    command: >
      --model /models/llama-3.1-8b-q4.gguf
      --host 0.0.0.0
      --port 8080
      --n-gpu-layers 99
      --ctx-size 2048
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # Ollama
  ollama:
    image: ollama/ollama:latest
    container_name: llamabench-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # vLLM OpenAI-compatible server
  vllm:
    image: vllm/vllm-openai:latest
    container_name: llamabench-vllm
    ports:
      - "8000:8000"
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: >
      --model meta-llama/Meta-Llama-3.1-8B-Instruct
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization 0.9
      --max-model-len 2048
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    shm_size: '2gb'

volumes:
  ollama-data:
    driver: local
