name: Benchmark Regression Tests

on:
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly benchmarks every Monday at 2 AM UTC
    - cron: '0 2 * * 1'
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Run benchmarks
      run: |
        python llamabench.py run \
          --model llama-3.1-8b \
          --engines llama.cpp,ollama \
          --concurrency 1,5 \
          --duration 30 \
          --output benchmark_results.json
    
    - name: Check for performance regression
      run: |
        python scripts/check_regression.py \
          --current benchmark_results.json \
          --baseline baseline_results.json \
          --threshold 10
    
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark_results.json
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const results = JSON.parse(fs.readFileSync('benchmark_results.json', 'utf8'));
          
          let comment = '## ðŸ¦™ llamabench Results\n\n';
          comment += '| Engine | Concurrency | TTFT | Throughput |\n';
          comment += '|--------|-------------|------|------------|\n';
          
          results.benchmarks.forEach(bench => {
            comment += `| ${bench.engine} | ${bench.concurrency} | ${bench.metrics.ttft_p50}s | ${bench.metrics.tokens_per_sec} tok/s |\n`;
          });
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  publish-results:
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'schedule'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Download results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results
    
    - name: Publish to GitHub Pages
      run: |
        # Script to update historical benchmark data
        python scripts/update_history.py benchmark_results.json
        
    - name: Commit results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add docs/benchmark_history.json
        git commit -m "Update benchmark history [skip ci]"
        git push
